from __future__ import annotations

import http
import itertools
import re
from datetime import UTC, datetime, timedelta
from hashlib import sha256
from typing import TYPE_CHECKING, ClassVar, Literal, NotRequired, TypedDict, TypeGuard

from cyberdrop_dl.crawlers.crawler import Crawler, RateLimit, SupportedPaths
from cyberdrop_dl.data_structures.url_objects import FILE_HOST_ALBUM, AbsoluteHttpURL, ScrapeItem
from cyberdrop_dl.exceptions import DownloadError, PasswordProtectedError, ScrapeError
from cyberdrop_dl.utils.utilities import error_handling_wrapper

if TYPE_CHECKING:
    from collections.abc import AsyncGenerator

    from typing_extensions import ReadOnly


_FIND_WT = re.compile(r'appdata\.wt\s=\s"([^"]+)"').search
_API_ENTRYPOINT = AbsoluteHttpURL("https://api.gofile.io")
_GLOBAL_JS_URL = AbsoluteHttpURL("https://gofile.io/dist/js/global.js")
_PRIMARY_URL = AbsoluteHttpURL("https://gofile.io")
_PER_PAGE: int = 1000
_TOKEN_MIN_AGE: timedelta = timedelta(seconds=120)


class Response(TypedDict):
    status: str


class Node(TypedDict):
    canAccess: ReadOnly[bool]
    id: str
    type: ReadOnly[Literal["folder", "file"]]
    name: str
    createTime: int

    # public: bool


class UnlockedNode(Node):
    canAccess: Literal[True]


class File(UnlockedNode):
    type: Literal["file"]
    link: str
    directLink: NotRequired[str]  # Only present in overloaded files (imported)
    isFrozen: NotRequired[bool]  # Only present in files uploaded by free accounts and older than 30 days
    viruses: NotRequired[bool]
    md5: str

    # parentFolder: str
    # size: int
    # downloadCount: int
    # thumbnail: str


class Folder(UnlockedNode):
    type: Literal["folder"]
    code: str
    childrenCount: int
    children: dict[str, Node]
    password: NotRequired[str]
    passwordStatus: NotRequired[str]

    # isRoot: NotRequired[bool]


class FolderMetadata(TypedDict):
    hasNextPage: bool

    # totalCount: int
    # totalPages: int
    # page: int
    # pageSize: int


class FolderResponse(Response):
    data: Folder
    metadata: FolderMetadata


class GoFileCrawler(Crawler):
    SUPPORTED_PATHS: ClassVar[SupportedPaths] = {
        "Folder / File": "/d/<content_id>",
        "Direct link": (
            "/download/<content_id>",
            "/download/web/<content_id>",
        ),
        "**NOTE**": (
            "Use `password` as a query param to download password protected folders",
            "ex: https://gofile.io/d/ABC654?password=1234",
        ),
    }
    PRIMARY_URL: ClassVar[AbsoluteHttpURL] = _PRIMARY_URL
    DOMAIN: ClassVar[str] = "gofile"
    FOLDER_DOMAIN: ClassVar[str] = "GoFile"
    _RATE_LIMIT: ClassVar[RateLimit] = 4, 10

    def __post_init__(self) -> None:
        self.api_key = self.manager.config_manager.authentication_data.gofile.api_key
        self.website_token = self.manager.cache_manager.get("gofile_website_token")
        self.headers: dict[str, str] = {}
        self._website_token_date = datetime.now(UTC) - timedelta(days=7)

    @classmethod
    def _json_response_check(cls, json_resp: Response) -> None:
        if not isinstance(json_resp, dict):
            return
        if "notFound" in json_resp["status"]:
            raise ScrapeError(404)

    async def async_startup(self) -> None:
        await self._get_account_token(_API_ENTRYPOINT)

    async def fetch(self, scrape_item: ScrapeItem) -> None:
        match scrape_item.url.parts[1:]:
            case ["d", content_id]:
                return await self.folder(scrape_item, content_id)
            case ["download", "web", file_id, _]:
                return await self.single_file(scrape_item, file_id)
            case ["download", file_id, _]:
                return await self.single_file(scrape_item, file_id)
            case _:
                raise ValueError

    @error_handling_wrapper
    async def single_file(self, scrape_item: ScrapeItem, file_id: str) -> None:
        url = await self._get_redirect_url(scrape_item.url)
        scrape_item.url = url.with_fragment(file_id)
        assert "d" in url.parts
        return await self.folder(scrape_item, url.name, file_id)

    @error_handling_wrapper
    async def folder(self, scrape_item: ScrapeItem, content_id: str, single_file_id: str | None = None) -> None:
        is_first_page: bool = True

        async for folder in self._folder_pager(content_id, scrape_item.url.query.get("password")):
            if is_first_page:
                if _has_single_not_nested_file(scrape_item, folder):
                    # Consider this file a loose file (autogenerated folder name)
                    title = ""
                    part_of_album = False
                else:
                    title = self.create_title(folder["name"], content_id)
                    part_of_album = True

                scrape_item.setup_as_album(title, album_id=content_id)
                scrape_item.part_of_album = part_of_album
                scrape_item.url = scrape_item.url.with_query(None)
                is_first_page = False

            children = folder["children"]
            if single_file_id:
                file = children.get(single_file_id)
                if not file:
                    continue

                children = {single_file_id: file}

            self._handle_children(scrape_item, children)

    def _handle_children(self, scrape_item: ScrapeItem, children: dict[str, Node]) -> None:
        def get_website_url(node: Node) -> AbsoluteHttpURL:
            node_id = node["id"]
            if node["type"] == "folder":
                return _PRIMARY_URL / "d" / (node.get("code") or node_id)
            return scrape_item.url.with_fragment(node_id)

        for node in children.values():
            web_url = get_website_url(node)
            new_scrape_item = scrape_item.create_child(web_url)
            self._handle_node(new_scrape_item, node)
            scrape_item.add_children()

    @error_handling_wrapper
    def _handle_node(self, scrape_item: ScrapeItem, node: Node) -> None:
        if not _check_node_is_accessible(node):
            return

        if node["type"] == "folder":
            self.create_task(self.run(scrape_item))
            return

        self.create_task(self._file(scrape_item, node))

    async def _folder_pager(self, content_id: str, password: str | None = None) -> AsyncGenerator[Folder]:
        api_url = (_API_ENTRYPOINT / "contents" / content_id).with_query(wt=self.website_token, pageSize=_PER_PAGE)

        if password:
            sha256_password = sha256(password.encode()).hexdigest()
            api_url = api_url.update_query(password=sha256_password)

        for page in itertools.count(1):
            resp = await self._request_folder(api_url.update_query(page=page))
            folder = resp["data"]
            _check_node_is_accessible(folder)
            yield folder
            if not resp["metadata"]["hasNextPage"]:
                break

    async def _request_folder(self, api_url: AbsoluteHttpURL) -> FolderResponse:
        try:
            resp: FolderResponse = await self.request_json(api_url, headers=self.headers)

        except DownloadError as e:
            if e.status != http.HTTPStatus.UNAUTHORIZED:
                raise

            async with self.startup_lock:
                await self.get_website_token(update=True)

            api_url = api_url.update_query(wt=self.website_token)
            resp = await self.request_json(api_url, headers=self.headers)

        self._json_response_check(resp)
        return resp

    @error_handling_wrapper
    async def _file(self, scrape_item: ScrapeItem, file: File) -> None:
        link_str: str = file["link"]
        if (not link_str or link_str == "overloaded") and "directLink" in file:
            link_str = file["directLink"]

        assert link_str
        link = self.parse_url(link_str)

        if await self.check_complete_by_hash(link, "md5", file["md5"]):
            return

        if file.get("isFrozen"):
            self.log(f"{link} is marked as frozen, download may fail", 30)

        name = file["name"]
        filename, ext = self.get_filename_and_ext(name, assume_ext=".mp4")
        scrape_item.possible_datetime = file["createTime"]
        await self.handle_file(link, scrape_item, name, ext, custom_filename=filename)

    @error_handling_wrapper
    async def _get_account_token(self, _) -> None:
        """Gets the token for the API."""
        with self.disable_on_error("Unable to get website token"):
            self.api_key = self.api_key or await self._get_new_api_key()
            self.headers["Authorization"] = f"Bearer {self.api_key}"
            cookies = {"accountToken": self.api_key}
            self.update_cookies(cookies)
            await self.get_website_token()

    async def _get_new_api_key(self) -> str:
        api_url = _API_ENTRYPOINT / "accounts"
        json_resp = await self.request_json(api_url, method="POST", data={})
        if json_resp["status"] != "ok":
            raise ScrapeError(401, "Couldn't generate GoFile API token", origin=api_url)

        return json_resp["data"]["token"]

    async def get_website_token(self, update: bool = False) -> None:
        """Creates an anon GoFile account to use."""
        if datetime.now(UTC) - self._website_token_date < _TOKEN_MIN_AGE:
            return
        if update:
            self.website_token = ""
            self.manager.cache_manager.remove("gofile_website_token")
        if self.website_token:
            return
        await self._update_website_token()

    async def _update_website_token(self) -> None:
        text = await self.request_text(_GLOBAL_JS_URL)
        match = _FIND_WT(text)
        if not match:
            raise ScrapeError(401, "Couldn't generate GoFile websiteToken", origin=_GLOBAL_JS_URL)

        self.website_token = match.group(1)
        self.manager.cache_manager.save("gofile_website_token", self.website_token)
        self._website_token_date = datetime.now(UTC)


def _check_node_is_accessible(node: Node) -> TypeGuard[File | Folder]:
    if (type_ := node["type"]) not in ("file", "folder"):
        raise ScrapeError(f"Unknown node type: {type_}")

    if node.get("viruses"):
        raise ScrapeError("Dangerous File")

    if node["canAccess"]:
        return True

    if node.get("password"):
        status = node.get("passwordStatus", "")
        error_msg = {
            "passwordRequired": "Folder is password protected",
            "passwordWrong": "Wrong folder password",
        }.get(status)
        raise PasswordProtectedError(error_msg)

    raise ScrapeError(403, "Folder is private")


def _has_single_not_nested_file(scrape_item: ScrapeItem, folder: Folder) -> bool:
    return folder["childrenCount"] == 1 and folder["name"] == folder["code"] and scrape_item.type != FILE_HOST_ALBUM
