from __future__ import annotations

import http
import itertools
import re
from datetime import UTC, datetime, timedelta
from hashlib import sha256
from typing import TYPE_CHECKING, Any, ClassVar, Literal, NotRequired, TypedDict, cast

from cyberdrop_dl.crawlers.crawler import Crawler, RateLimit, SupportedPaths
from cyberdrop_dl.data_structures.url_objects import FILE_HOST_ALBUM, AbsoluteHttpURL, ScrapeItem
from cyberdrop_dl.exceptions import DownloadError, PasswordProtectedError, ScrapeError
from cyberdrop_dl.utils.utilities import error_handling_wrapper

if TYPE_CHECKING:
    from collections.abc import AsyncGenerator, Mapping

    from typing_extensions import ReadOnly


_FIND_WT = re.compile(r'appdata\.wt\s=\s"([^"]+)"').search
_API_ENTRYPOINT = AbsoluteHttpURL("https://api.gofile.io")
_GLOBAL_JS_URL = AbsoluteHttpURL("https://gofile.io/dist/js/global.js")
_PRIMARY_URL = AbsoluteHttpURL("https://gofile.io")
_PER_PAGE: int = 1000


class Node(TypedDict):
    canAccess: ReadOnly[bool]
    id: str
    type: ReadOnly[Literal["folder", "file"]]
    name: str
    createTime: int

    # Not used
    public: bool


class UnlockedNode(Node):
    canAccess: Literal[True]
    code: str


class UnlockedFile(UnlockedNode):
    type: Literal["file"]
    link: str
    directLink: NotRequired[str]  # Only present in overloded files (imported)
    isFrozen: NotRequired[bool]  # Only present in files uploaded by free accounts and older than 30 days

    # Not used
    parentFolder: str
    size: int
    downloadCount: int
    md5: str
    thumbnail: str


class UnlockedFolder(UnlockedNode):
    type: Literal["folder"]

    # Not used
    isRoot: NotRequired[bool]


class Album(UnlockedFolder):
    childrenCount: int
    children: dict[str, Node]
    password: NotRequired[str]


class AlbumMetadata(TypedDict):
    totalCount: int
    totalPages: int
    page: int
    pageSize: int
    hasNextPage: bool


class ApiAlbumResponse(TypedDict):
    status: str
    data: Album
    metadata: AlbumMetadata


class GoFileCrawler(Crawler):
    SUPPORTED_PATHS: ClassVar[SupportedPaths] = {"Album": "/d/..."}
    PRIMARY_URL: ClassVar[AbsoluteHttpURL] = _PRIMARY_URL
    DOMAIN: ClassVar[str] = "gofile"
    FOLDER_DOMAIN: ClassVar[str] = "GoFile"
    _RATE_LIMIT: ClassVar[RateLimit] = 4, 10

    def __post_init__(self) -> None:
        self.api_key = self.manager.config_manager.authentication_data.gofile.api_key
        self.website_token = self.manager.cache_manager.get("gofile_website_token")
        self.headers: dict[str, str] = {}
        self._website_token_date = datetime.now(UTC) - timedelta(days=7)

    @classmethod
    def _json_response_check(cls, json_resp: Any) -> None:
        if not isinstance(json_resp, dict):
            return
        if "notFound" in json_resp["status"]:
            raise ScrapeError(404)

    async def async_startup(self) -> None:
        await self.get_account_token(_API_ENTRYPOINT)

    async def fetch(self, scrape_item: ScrapeItem) -> None:
        match scrape_item.url.parts[1:]:
            case ["d", content_id]:
                return await self.album(scrape_item, content_id)
            case _:
                raise ValueError

    @error_handling_wrapper
    async def album(self, scrape_item: ScrapeItem, content_id: str) -> None:
        if not self.api_key or not self.website_token:
            return

        is_first_page: bool = True
        async for album in self._album_pager(content_id, scrape_item.url.query.get("password")):
            if is_first_page:
                if _is_single_not_nested_file(scrape_item, album):
                    # Consider this file a loose file (autogenerated album name)
                    title = ""
                    part_of_album = False
                else:
                    title = self.create_title(album["name"], content_id)
                    part_of_album = True

                scrape_item.setup_as_album(title, album_id=content_id)
                scrape_item.part_of_album = part_of_album
                scrape_item.url = scrape_item.url.with_query(None)
                is_first_page = False

            self._handle_children(scrape_item, album["children"])

    async def _album_pager(self, content_id: str, password: str | None = None) -> AsyncGenerator[Album]:
        api_url = (_API_ENTRYPOINT / "contents" / content_id).with_query(wt=self.website_token, pageSize=_PER_PAGE)

        if password:
            sha256_password = sha256(password.encode()).hexdigest()
            api_url = api_url.update_query(password=sha256_password)

        for page in itertools.count(1):
            resp = await self._request_album(api_url.update_query(page=page))
            yield resp["data"]
            if not resp["metadata"]["hasNextPage"]:
                break

    async def _request_album(self, api_url: AbsoluteHttpURL) -> ApiAlbumResponse:
        try:
            json_resp: ApiAlbumResponse = await self.request_json(api_url, headers=self.headers)

        except DownloadError as e:
            if e.status != http.HTTPStatus.UNAUTHORIZED:
                raise

            async with self.startup_lock:
                await self.get_website_token(update=True)

            api_url = api_url.update_query(wt=self.website_token)
            json_resp = await self.request_json(api_url, headers=self.headers)

        self._json_response_check(json_resp)
        _check_album_response(json_resp)
        return json_resp

    def _handle_children(self, scrape_item: ScrapeItem, children: Mapping[str, Node]) -> None:
        """Sends files to downloader and adds subfolder to scrape queue."""

        def get_website_url(node: Node) -> AbsoluteHttpURL:
            if node["type"] == "folder":
                return _PRIMARY_URL / "d" / (node.get("code") or node["id"])
            return scrape_item.url.with_fragment(node["id"])

        for child in children.values():
            web_url = get_website_url(child)
            if not child["canAccess"]:
                self.raise_exc(scrape_item.create_child(web_url), ScrapeError(403))
                continue

            if child.get("v" + "iru" + "ses"):  # Auto flagged by GoFile. We can download them but better not to
                self.raise_exc(scrape_item.create_child(web_url), ScrapeError("Dangerous File"))
                continue

            if child["type"] == "folder":
                self.create_task(self.run(scrape_item.create_child(web_url)))
                scrape_item.add_children()
                continue

            assert child["type"] == "file"
            file = cast("UnlockedFile", child)
            self._file(scrape_item, file)
            scrape_item.add_children()

    @error_handling_wrapper
    def _file(self, scrape_item: ScrapeItem, file: UnlockedFile) -> None:
        link_str: str = file["link"]
        if (not link_str or link_str == "overloaded") and "directLink" in file:
            link_str = file["directLink"]

        assert link_str
        link = self.parse_url(link_str)

        if file.get("isFrozen"):
            self.log(f"{link} is marked as frozen, download may fail", 30)

        name = file["name"]
        filename, ext = self.get_filename_and_ext(name, assume_ext=".mp4")
        new_scrape_item = scrape_item.copy()
        new_scrape_item.possible_datetime = file["createTime"]
        self.create_task(self.handle_file(link, new_scrape_item, name, ext, custom_filename=filename))

    @error_handling_wrapper
    async def get_account_token(self, _) -> None:
        """Gets the token for the API."""
        self.api_key = self.api_key or await self._get_new_api_key()
        self.headers["Authorization"] = f"Bearer {self.api_key}"
        cookies = {"accountToken": self.api_key}
        self.update_cookies(cookies)
        await self.get_website_token()

    async def _get_new_api_key(self) -> str:
        api_url = _API_ENTRYPOINT / "accounts"
        json_resp = await self.request_json(api_url, method="POST", data={})
        if json_resp["status"] != "ok":
            raise ScrapeError(401, "Couldn't generate GoFile API token", origin=api_url)

        return json_resp["data"]["token"]

    async def get_website_token(self, update: bool = False) -> None:
        """Creates an anon GoFile account to use."""
        if datetime.now(UTC) - self._website_token_date < timedelta(seconds=120):
            return
        if update:
            self.website_token = ""
            self.manager.cache_manager.remove("gofile_website_token")
        if self.website_token:
            return
        await self._update_website_token()

    async def _update_website_token(self) -> None:
        text = await self.request_text(_GLOBAL_JS_URL)
        match = _FIND_WT(text)
        if not match:
            raise ScrapeError(401, "Couldn't generate GoFile websiteToken", origin=_GLOBAL_JS_URL)

        self.website_token = match.group(1)
        self.manager.cache_manager.save("gofile_website_token", self.website_token)
        self._website_token_date = datetime.now(UTC)


def _check_album_response(json_resp: ApiAlbumResponse) -> None:
    """Parses and raises errors if we can not proccess the API response."""

    album: Album = json_resp["data"]
    if (password := album.get("password")) and (password in ("passwordRequired", "passwordWrong")):
        raise PasswordProtectedError(password)

    if not album["canAccess"]:
        raise ScrapeError(403, "Album is private")


def _is_single_not_nested_file(scrape_item: ScrapeItem, album: Album) -> bool:
    return album["childrenCount"] == 1 and album["name"] == album["code"] and scrape_item.type != FILE_HOST_ALBUM
