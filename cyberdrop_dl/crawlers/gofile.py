from __future__ import annotations

import http
import re
from datetime import UTC, datetime, timedelta
from hashlib import sha256
from typing import TYPE_CHECKING, ClassVar, Literal, NotRequired, TypedDict, cast

from aiolimiter import AsyncLimiter

from cyberdrop_dl.crawlers.crawler import Crawler
from cyberdrop_dl.data_structures.url_objects import FILE_HOST_ALBUM, ScrapeItem
from cyberdrop_dl.exceptions import DownloadError, PasswordProtectedError, ScrapeError
from cyberdrop_dl.types import AbsoluteHttpURL, SupportedPaths
from cyberdrop_dl.utils.utilities import error_handling_wrapper

if TYPE_CHECKING:
    from collections.abc import Mapping

    from yarl import URL


WT_REGEX = re.compile(r'appdata\.wt\s=\s"([^"]+)"')
API_ENTRYPOINT = AbsoluteHttpURL("https://api.gofile.io")
GLOBAL_JS_URL = AbsoluteHttpURL("https://gofile.io/dist/js/global.js")
PRIMARY_URL = AbsoluteHttpURL("https://gofile.io")


class Node(TypedDict):
    canAccess: bool
    id: str
    type: Literal["folder", "file"]
    name: str
    createTime: int

    # Not used
    public: bool


class UnlockedNode(Node):
    canAccess: Literal[True]
    code: str


class UnlockedFile(UnlockedNode):
    type: Literal["file"]
    link: str
    directLink: NotRequired[str]  # Only present in overloded files (imported)
    isFrozen: NotRequired[bool]  # Only present in files uploaded by free accounts and older than 30 days

    # Not used
    parentFolder: str
    size: int
    downloadCount: int
    md5: str
    thumbnail: str


class UnlockedFolder(UnlockedNode):
    type: Literal["folder"]

    # Not used
    isRoot: NotRequired[bool]


class Album(UnlockedFolder):
    childrenCount: int
    children: dict[str, Node]
    password: NotRequired[str]


# TODO: add pagination support for albums with 1000+ items
class AlbumMetadata(TypedDict):
    totalCount: int
    totalPages: int
    page: int
    pageSize: Literal[1000]
    hasNextPage: bool


class ApiAlbumResponse(TypedDict):
    status: str
    data: Album
    Metadata: AlbumMetadata


class GoFileCrawler(Crawler):
    SUPPORTED_PATHS: ClassVar[SupportedPaths] = {"Album": "/d/..."}
    PRIMARY_URL: ClassVar[AbsoluteHttpURL] = PRIMARY_URL
    DOMAIN: ClassVar[str] = "gofile"
    FOLDER_DOMAIN: ClassVar[str] = "GoFile"

    def __post_init__(self) -> None:
        self.api_key = self.manager.config_manager.authentication_data.gofile.api_key
        self.website_token = self.manager.cache_manager.get("gofile_website_token")
        self.headers: dict[str, str] = {}
        self.request_limiter = AsyncLimiter(4, 6)
        self._website_token_date = datetime.now(UTC) - timedelta(days=7)

    async def async_startup(self) -> None:
        get_website_token = error_handling_wrapper(self.get_website_token)
        await self.get_account_token(API_ENTRYPOINT)
        await get_website_token(self, PRIMARY_URL)

    async def fetch(self, scrape_item: ScrapeItem) -> None:
        if "d" in scrape_item.url.parts:
            return await self.album(scrape_item)
        raise ValueError

    @error_handling_wrapper
    async def album(self, scrape_item: ScrapeItem) -> None:
        if not self.api_key or not self.website_token:
            return

        content_id = scrape_item.url.name
        api_url = API_ENTRYPOINT.joinpath("contents", content_id).with_query(wt=self.website_token)

        if password := scrape_item.url.query.get("password"):
            sha256_password = sha256(password.encode()).hexdigest()
            api_url = api_url.update_query(password=sha256_password)

        try:
            async with self.request_limiter:
                json_resp: ApiAlbumResponse = await self.client.get_json(self.DOMAIN, api_url, headers=self.headers)

        except DownloadError as e:
            if e.status != http.HTTPStatus.UNAUTHORIZED:
                raise
            async with self.startup_lock:
                await self.get_website_token(update=True)
            api_url = api_url.update_query(wt=self.website_token)
            async with self.request_limiter:
                json_resp = await self.client.get_json(self.DOMAIN, api_url, headers=self.headers)

        album = get_album_data(json_resp)
        if is_single_not_nested_file(scrape_item, album):
            # Consider this file a loose file (autogenerated album name)
            title = ""
            part_of_album = False
        else:
            title = self.create_title(album["name"], content_id)
            part_of_album = True

        scrape_item.setup_as_album(title, album_id=content_id)
        scrape_item.part_of_album = part_of_album
        scrape_item.url = scrape_item.url.with_query(None)
        await self.handle_children(album["children"], scrape_item)

    async def handle_children(self, children: Mapping[str, Node], scrape_item: ScrapeItem) -> None:
        """Sends files to downloader and adds subfolder to scrape queue."""
        subfolders: list[URL] = []
        unavailable: list[URL] = []
        dangerous: list[URL] = []

        def get_website_url(node: Node) -> URL:
            if node["type"] == "folder":
                return PRIMARY_URL / "d" / (node.get("code") or node["id"])
            return scrape_item.url.with_fragment(file["id"])

        for child in children.values():
            if not child["canAccess"]:
                url = get_website_url(child)
                unavailable.append(url)
                continue

            if child["type"] == "folder":
                folder = cast("UnlockedFolder", child)
                url = PRIMARY_URL / "d" / folder["code"]
                subfolders.append(url)
                continue

            assert child["type"] == "file"
            file = cast("UnlockedFile", child)

            link_str = file["link"]
            if not link_str or link_str == "overloaded":
                link_str = file.get("directLink")
                assert link_str

            link = self.parse_url(link_str)

            if file.get("v" + "iru" + "ses"):  # Auto flagged by GoFile. We can download them but better not to
                dangerous.append(link)
                continue

            if file.get("isFrozen"):
                self.log(f"{link} is marked as frozen, download may fail", 30)

            filename, ext = self.get_filename_and_ext(link.name, assume_ext=".mp4")
            new_scrape_item = scrape_item.create_new(scrape_item.url, possible_datetime=file["createTime"])
            await self.handle_file(link, new_scrape_item, filename, ext)
            scrape_item.add_children()

        for url in subfolders:
            subfolder = scrape_item.create_child(url)
            self.manager.task_group.create_task(self.run(subfolder))

        for url in unavailable:
            new_scrape_item = scrape_item.create_new(url)
            await self.raise_error(new_scrape_item, 403)

        for url in dangerous:
            new_scrape_item = scrape_item.create_new(url)
            await self.raise_error(new_scrape_item, "Dangerous File")

    @error_handling_wrapper
    async def raise_error(self, scrape_item: ScrapeItem | URL, status: str | int, message: str | None = None) -> None:
        raise ScrapeError(status, message)

    @error_handling_wrapper
    async def get_account_token(self, _) -> None:
        """Gets the token for the API."""
        self.api_key = self.api_key or await self._get_new_api_key()
        self.headers["Authorization"] = f"Bearer {self.api_key}"
        cookies = {"accountToken": self.api_key}
        self.update_cookies(cookies)

    async def _get_new_api_key(self) -> str:
        api_url = API_ENTRYPOINT / "accounts"
        async with self.request_limiter:
            json_resp = await self.client.post_data(self.DOMAIN, api_url, data={})
        if json_resp["status"] != "ok":
            raise ScrapeError(401, "Couldn't generate GoFile API token", origin=api_url)

        return json_resp["data"]["token"]

    async def get_website_token(self, _: ScrapeItem | URL | None = None, update: bool = False) -> None:
        """Creates an anon GoFile account to use."""
        if datetime.now(UTC) - self._website_token_date < timedelta(seconds=120):
            return
        if update:
            self.website_token = ""
            self.manager.cache_manager.remove("gofile_website_token")
        if self.website_token:
            return
        await self._update_website_token()

    async def _update_website_token(self) -> None:
        async with self.request_limiter:
            text = await self.client.get_text(self.DOMAIN, GLOBAL_JS_URL)
        match = WT_REGEX.search(str(text))
        if not match:
            raise ScrapeError(401, "Couldn't generate GoFile websiteToken", origin=GLOBAL_JS_URL)
        self.website_token = match.group(1)
        self.manager.cache_manager.save("gofile_website_token", self.website_token)
        self._website_token_date = datetime.now(UTC)


def get_album_data(json_resp: ApiAlbumResponse) -> Album:
    """Parses and raises errors if we can not proccess the API response."""
    if json_resp["status"] == "error-notFound":
        raise ScrapeError(404)

    album: Album = json_resp["data"]
    if (password := album.get("password")) and (password in ("passwordRequired", "passwordWrong")):
        raise PasswordProtectedError

    if not album["canAccess"]:
        raise ScrapeError(403, "Album is private")

    return album


def is_single_not_nested_file(scrape_item: ScrapeItem, album: Album) -> bool:
    return album["childrenCount"] == 1 and album["name"] == album["code"] and scrape_item.type != FILE_HOST_ALBUM
